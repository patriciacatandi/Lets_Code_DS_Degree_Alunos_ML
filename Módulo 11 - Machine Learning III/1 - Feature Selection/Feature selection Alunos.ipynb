{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula - Feature Selection\n",
    "\n",
    "Nas próximas duas aulas vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Introdução\n",
    "- 2) Métodos de seleção de features supervisonado\n",
    "- 3) Métodos de filtro\n",
    "- 4) Métodos wrapper\n",
    "- 5) Métodos híbridos\n",
    "- 6) Médotos embutidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:45.719993Z",
     "start_time": "2022-03-11T22:18:35.104865Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:45.751977Z",
     "start_time": "2022-03-11T22:18:45.725992Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def metricas_classificacao(estimator, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de treino:\")\n",
    "\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_train, y_pred_train)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de teste:\")\n",
    "\n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o dataset Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# Load do Dataset Breast Cancer\n",
    "bc = datasets.load_breast_cancer(as_frame=True)\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# Cria training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>11.62</td>\n",
       "      <td>18.18</td>\n",
       "      <td>76.38</td>\n",
       "      <td>408.8</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>0.05564</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.07255</td>\n",
       "      <td>...</td>\n",
       "      <td>13.36</td>\n",
       "      <td>25.40</td>\n",
       "      <td>88.14</td>\n",
       "      <td>528.1</td>\n",
       "      <td>0.17800</td>\n",
       "      <td>0.28780</td>\n",
       "      <td>0.31860</td>\n",
       "      <td>0.14160</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.09270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.20</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>10.57</td>\n",
       "      <td>18.32</td>\n",
       "      <td>66.82</td>\n",
       "      <td>340.9</td>\n",
       "      <td>0.08142</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.01993</td>\n",
       "      <td>0.01111</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.05768</td>\n",
       "      <td>...</td>\n",
       "      <td>10.94</td>\n",
       "      <td>23.31</td>\n",
       "      <td>69.35</td>\n",
       "      <td>366.3</td>\n",
       "      <td>0.09794</td>\n",
       "      <td>0.06542</td>\n",
       "      <td>0.03986</td>\n",
       "      <td>0.02222</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.06736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>12.76</td>\n",
       "      <td>18.84</td>\n",
       "      <td>81.87</td>\n",
       "      <td>496.6</td>\n",
       "      <td>0.09676</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.01781</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.06183</td>\n",
       "      <td>...</td>\n",
       "      <td>13.75</td>\n",
       "      <td>25.99</td>\n",
       "      <td>87.82</td>\n",
       "      <td>579.7</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.18390</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.08312</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.07238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>11.29</td>\n",
       "      <td>13.04</td>\n",
       "      <td>72.23</td>\n",
       "      <td>388.0</td>\n",
       "      <td>0.09834</td>\n",
       "      <td>0.07608</td>\n",
       "      <td>0.03265</td>\n",
       "      <td>0.02755</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.06270</td>\n",
       "      <td>...</td>\n",
       "      <td>12.32</td>\n",
       "      <td>16.18</td>\n",
       "      <td>78.27</td>\n",
       "      <td>457.5</td>\n",
       "      <td>0.13580</td>\n",
       "      <td>0.15070</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.08750</td>\n",
       "      <td>0.2733</td>\n",
       "      <td>0.08022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "469        11.62         18.18           76.38      408.8          0.11750   \n",
       "561        11.20         29.37           70.67      386.0          0.07449   \n",
       "443        10.57         18.32           66.82      340.9          0.08142   \n",
       "362        12.76         18.84           81.87      496.6          0.09676   \n",
       "271        11.29         13.04           72.23      388.0          0.09834   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "469           0.14830         0.10200              0.05564         0.1957   \n",
       "561           0.03558         0.00000              0.00000         0.1060   \n",
       "443           0.04462         0.01993              0.01111         0.2372   \n",
       "362           0.07952         0.02688              0.01781         0.1759   \n",
       "271           0.07608         0.03265              0.02755         0.1769   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "469                 0.07255  ...         13.36          25.40   \n",
       "561                 0.05502  ...         11.92          38.30   \n",
       "443                 0.05768  ...         10.94          23.31   \n",
       "362                 0.06183  ...         13.75          25.99   \n",
       "271                 0.06270  ...         12.32          16.18   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "469            88.14       528.1           0.17800            0.28780   \n",
       "561            75.19       439.6           0.09267            0.05494   \n",
       "443            69.35       366.3           0.09794            0.06542   \n",
       "362            87.82       579.7           0.12980            0.18390   \n",
       "271            78.27       457.5           0.13580            0.15070   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "469          0.31860               0.14160          0.2660   \n",
       "561          0.00000               0.00000          0.1566   \n",
       "443          0.03986               0.02222          0.2699   \n",
       "362          0.12550               0.08312          0.2744   \n",
       "271          0.12750               0.08750          0.2733   \n",
       "\n",
       "     worst fractal dimension  \n",
       "469                  0.09270  \n",
       "561                  0.05905  \n",
       "443                  0.06736  \n",
       "362                  0.07238  \n",
       "271                  0.08022  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 398 entries, 469 to 176\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              398 non-null    float64\n",
      " 1   mean texture             398 non-null    float64\n",
      " 2   mean perimeter           398 non-null    float64\n",
      " 3   mean area                398 non-null    float64\n",
      " 4   mean smoothness          398 non-null    float64\n",
      " 5   mean compactness         398 non-null    float64\n",
      " 6   mean concavity           398 non-null    float64\n",
      " 7   mean concave points      398 non-null    float64\n",
      " 8   mean symmetry            398 non-null    float64\n",
      " 9   mean fractal dimension   398 non-null    float64\n",
      " 10  radius error             398 non-null    float64\n",
      " 11  texture error            398 non-null    float64\n",
      " 12  perimeter error          398 non-null    float64\n",
      " 13  area error               398 non-null    float64\n",
      " 14  smoothness error         398 non-null    float64\n",
      " 15  compactness error        398 non-null    float64\n",
      " 16  concavity error          398 non-null    float64\n",
      " 17  concave points error     398 non-null    float64\n",
      " 18  symmetry error           398 non-null    float64\n",
      " 19  fractal dimension error  398 non-null    float64\n",
      " 20  worst radius             398 non-null    float64\n",
      " 21  worst texture            398 non-null    float64\n",
      " 22  worst perimeter          398 non-null    float64\n",
      " 23  worst area               398 non-null    float64\n",
      " 24  worst smoothness         398 non-null    float64\n",
      " 25  worst compactness        398 non-null    float64\n",
      " 26  worst concavity          398 non-null    float64\n",
      " 27  worst concave points     398 non-null    float64\n",
      " 28  worst symmetry           398 non-null    float64\n",
      " 29  worst fractal dimension  398 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 96.4 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introdução\n",
    "\n",
    "O processo de **feature selection** (**seleção de atributos**) consiste na escolha, com base em alguns critérios, de um **subconjunto do conjunto original** de features de um dado problema que proporcionem um modelo com performance comparável ao modelo treinado com todas as features. \n",
    "\n",
    "<img src=https://miro.medium.com/max/694/0*D_jQ5yBsvCZjEYIW width=400>\n",
    "\n",
    "O resultado do processo de feature selection é uma **redução na dimensionalidade** do espaço de features do problema (mas aqui, diferente do PCA, trabalhamos no espaço de features originais!)\n",
    "\n",
    "Assim, o processo remove features redundantes ou irrelevantes. \n",
    "\n",
    "Dentre as vantagens do procedimento, podemos destacar:\n",
    "\n",
    "- Maior eficiência no treinamento (afinal, reduzimos a quantidade de informação a ser processada);\n",
    "- Eliminação de redundâncias (como multicolinearidade, por exemplo, que pode ser problemática para alguns estimadores);\n",
    "- Um modelo com menos features é, em geral, mais facilmente interpretável;\n",
    "- Ao reduzirmos o número de features, a complexidade da hipótese é reduzida, o que pode favorecer a generalização e melhorar a predição do estimador;\n",
    "\n",
    "\n",
    "O princípio da [navalha de Occam](https://pt.wikipedia.org/wiki/Navalha_de_Ockham) é relevante no contexto de feature selection em projetos de machine learning. Sugiro [este post](https://machinelearningmastery.com/ensemble-learning-and-occams-razor/#:~:text=Occam's%20razor%20suggests%20that%20in,narrow%20and%20not%20generalize%20well.) para uma discussão deste princípio como uma heurística para a construção de modelos. Para uma discussão mais profunda, sugiro [este paper](https://www.aaai.org/Papers/KDD/1998/KDD98-006.pdf).\n",
    "\n",
    "Alguns modelos que são sensíveis à atributos irrelevantes:\n",
    " - Regressão Linear e Logística (principalmente se forem correlacionados)\n",
    " - KNN\n",
    " - SVM\n",
    " - Redes Neurais\n",
    "\n",
    "## Etapas do pré-processamento dos dados\n",
    "\n",
    "<img src=\"images/pre-processing_order.png\" width=700>\n",
    "\n",
    "## Tipos de métodos de feature selection\n",
    "Assim como temos modelos supervisionados e não supervisionados, teremos técnicas de seleção de feature que dependem da variável target ou não:\n",
    "\n",
    "<img src='https://www.kdnuggets.com/wp-content/uploads/Fig1-Butvinik-feature-selection-overview.jpg' text='https://www.kdnuggets.com/2021/06/feature-selection-overview.html'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Técnicas Supervisionadas\n",
    "Nesta aula, estudaremos alguns métodos de seleção de features que utilizam o target.\n",
    "\n",
    "\n",
    "Podemos classificar os métodos de seleção de features supervisionados de acordo com a sua interação com o modelo de aprendizado: \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/Fig3-Butvinik-feature-selection-overview.jpg\" text=\"https://www.kdnuggets.com/2021/06/feature-selection-overview.html\"/>\n",
    "\n",
    "\n",
    "### 1. Filtro\n",
    "\n",
    "Aqui, utilizamos **técnicas estatísticas** como ganho de informação, teste qui-quadrado, pontuação de Fisher e correlação com o target. As features são rankeadas de acordo com a técnica estatística escolhida e as melhores são selecionadas. Esse método **independe do estimador escolhido** e necessita de menos tempo computacional. A principal vantagem é que podemos trocar o estimador que as principais features serão as mesmas, não necessitando refazer essa etapa.\n",
    "\n",
    "A técnica estatística a ser escolhida dependerá do tipo da sua variável dependente, se o target é categórico ou contínuo, e dos tipos da suas variáveis independentes, se suas features são categóricas ou contínuas.\n",
    "\n",
    "<img src=\"images/How-to-Choose-Feature-Selection-Methods.png\" text='imagem modificada de: machinelearningmastery.com/feature-selection-with-real-and-categorical-data/' width=600px>\n",
    "\n",
    "Vamos agora ver um exemplo de filtro utilizando o \n",
    "[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) do sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o nosso caso, qual(is) desses filtros podemos utilizar?\n",
    "\n",
    "[ANOVA (**An**alysis **o**f **Va**riance)](https://towardsdatascience.com/statistics-in-python-using-anova-for-feature-selection-b4dc876ef4f0) pressupõe que você tem variáveis contínuas de um lado e categóricas do outro. Para avaliar as features mais importantes, ela compara os grupos categóricos analisando se há uma igual variância nos dados contínuos. Se para os diferentes grupos temos uma mesma variância, isso significa que essa feature contínua não é relevante para separar os grupos e, portanto, pode ser eliminada da modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de features antes: 30, quantidade de features depois 10\n"
     ]
    }
   ],
   "source": [
    "# importa SelectKBest e f_classif\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# instancia SelectKBest\n",
    "fs = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Cria novo df com fit_transform\n",
    "X_new = fs.fit_transform(X_train, y_train)\n",
    "\n",
    "print(f'Quantidade de features antes: {X_train.shape[1]}, quantidade de features depois {X_new.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.162e+01, 7.638e+01, 4.088e+02, ..., 5.281e+02, 3.186e-01,\n",
       "        1.416e-01],\n",
       "       [1.120e+01, 7.067e+01, 3.860e+02, ..., 4.396e+02, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.057e+01, 6.682e+01, 3.409e+02, ..., 3.663e+02, 3.986e-02,\n",
       "        2.222e-02],\n",
       "       ...,\n",
       "       [1.365e+01, 8.788e+01, 5.689e+02, ..., 7.062e+02, 1.759e-01,\n",
       "        8.056e-02],\n",
       "       [1.705e+01, 1.134e+02, 8.950e+02, ..., 1.189e+03, 5.018e-01,\n",
       "        2.543e-01],\n",
       "       [9.904e+00, 6.460e+01, 3.024e+02, ..., 3.902e+02, 3.486e-01,\n",
       "        9.910e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean perimeter', 'mean area', 'mean concavity',\n",
       "       'mean concave points', 'worst radius', 'worst perimeter',\n",
       "       'worst area', 'worst concavity', 'worst concave points'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_feature_names_out retorna o nome das features selecionadas\n",
    "fs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.80594009e+02, 7.30809534e+01, 5.19551421e+02, 4.50499138e+02,\n",
       "       6.82480951e+01, 2.35101305e+02, 3.65480510e+02, 6.35025749e+02,\n",
       "       5.24483545e+01, 5.31343724e-03, 2.22806961e+02, 1.27184749e+00,\n",
       "       2.06798349e+02, 2.78960274e+02, 5.34605383e+00, 3.51063285e+01,\n",
       "       2.36099869e+01, 7.68295227e+01, 5.00864231e-02, 7.85634972e-01,\n",
       "       6.17391336e+02, 9.84586408e+01, 6.43459905e+02, 4.86344354e+02,\n",
       "       9.34278899e+01, 2.37748913e+02, 3.35294070e+02, 7.32283255e+02,\n",
       "       9.58762662e+01, 5.19076118e+01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores_ retorna o score de cada uma das features\n",
    "fs.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean radius', 480.5940091197921)\n",
      "('mean texture', 73.08095339361246)\n",
      "('mean perimeter', 519.5514214259588)\n",
      "('mean area', 450.49913811976836)\n",
      "('mean smoothness', 68.24809509035939)\n",
      "('mean compactness', 235.10130508217816)\n",
      "('mean concavity', 365.4805099358985)\n",
      "('mean concave points', 635.0257490469319)\n",
      "('mean symmetry', 52.44835447488636)\n",
      "('mean fractal dimension', 0.00531343723989453)\n",
      "('radius error', 222.80696125355692)\n",
      "('texture error', 1.2718474925637684)\n",
      "('perimeter error', 206.79834937961834)\n",
      "('area error', 278.96027418566905)\n",
      "('smoothness error', 5.346053826057126)\n",
      "('compactness error', 35.10632849853479)\n",
      "('concavity error', 23.609986868535493)\n",
      "('concave points error', 76.82952268685312)\n",
      "('symmetry error', 0.05008642306429728)\n",
      "('fractal dimension error', 0.7856349715005921)\n",
      "('worst radius', 617.3913359033312)\n",
      "('worst texture', 98.45864081795534)\n",
      "('worst perimeter', 643.4599048186216)\n",
      "('worst area', 486.34435439438516)\n",
      "('worst smoothness', 93.42788988624137)\n",
      "('worst compactness', 237.74891309086067)\n",
      "('worst concavity', 335.29406970612325)\n",
      "('worst concave points', 732.2832548584621)\n",
      "('worst symmetry', 95.87626622181143)\n",
      "('worst fractal dimension', 51.90761175329877)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(X_train.columns, fs.scores_):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **informação mútua (MI)** entre duas variáveis ​​aleatórias é um valor não negativo, que mede a dependência entre as variáveis. É igual a zero se e somente se duas variáveis ​​aleatórias são independentes, e valores mais altos significam maior dependência.\n",
    "\n",
    "Utilize a classe [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) para criar um novo dataframe contendo apenas as 10 features mais relevantes segundo essa abordagem e print o nome dessas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/patricia/Documentos/LC/Lets_Code_DS_Degree_Alunos_ML/Módulo 11 - Machine Learning III/1 - Feature Selection/Feature selection Alunos.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/patricia/Documentos/LC/Lets_Code_DS_Degree_Alunos_ML/M%C3%B3dulo%2011%20-%20Machine%20Learning%20III/1%20-%20Feature%20Selection/Feature%20selection%20Alunos.ipynb#ch0000020?line=0'>1</a>\u001b[0m mi\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mi' is not defined"
     ]
    }
   ],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>mutual information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>worst perimeter</td>\n",
       "      <td>0.479360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>worst radius</td>\n",
       "      <td>0.456777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>worst concave points</td>\n",
       "      <td>0.456081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>worst area</td>\n",
       "      <td>0.453666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mean concave points</td>\n",
       "      <td>0.441987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean perimeter</td>\n",
       "      <td>0.408952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean radius</td>\n",
       "      <td>0.365615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean concavity</td>\n",
       "      <td>0.360477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean area</td>\n",
       "      <td>0.355586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>worst concavity</td>\n",
       "      <td>0.335757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature  mutual information\n",
       "22       worst perimeter            0.479360\n",
       "20          worst radius            0.456777\n",
       "27  worst concave points            0.456081\n",
       "23            worst area            0.453666\n",
       "7    mean concave points            0.441987\n",
       "2         mean perimeter            0.408952\n",
       "0            mean radius            0.365615\n",
       "6         mean concavity            0.360477\n",
       "3              mean area            0.355586\n",
       "26       worst concavity            0.335757"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "mi\n",
    "df_mi = pd.DataFrame(zip(X_train.columns, mi), columns = [\"Feature\",\"mutual information\"])\n",
    "df_mi.sort_values(by = [\"mutual information\"], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de features antes: 30, quantidade de features depois 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# instancia SelectKBest\n",
    "fs = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "\n",
    "# Cria novo df com fit_transform\n",
    "X_new = fs.fit_transform(X_train, y_train)\n",
    "\n",
    "print(f'Quantidade de features antes: {X_train.shape[1]}, quantidade de features depois {X_new.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrapper\n",
    "\n",
    "Nesse método, diferentes combinações de features são selecionadas, avaliadas utilizando-se um modelo e comparadas com as outras combinações com base dos resultados desse estimador. Dessa forma, a escolha das features depende do estimador escolhido e a **busca será feita em todas as possíveis combinações de features utilizando a métrica escolhida**.\n",
    "\n",
    "As estratégias mais conhecidos são: <br>\n",
    "* Forward selection - [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) <br>\n",
    "* Backward elimination - [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html), [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) <br>\n",
    "* Stepwise selection (Bi-directional elimination) - [SequentialFeatureSelector do mlxtend](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector)<br>\n",
    "* Permutation Importance - [permutation_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html)\n",
    "<br><br>\n",
    "\n",
    "__Foward Selection__\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Começa com um modelo que não contém variáveis (chamado de Null Model)\n",
    "2) Faz um modelo com cada uma das features separadamente\n",
    "3) Escolhe a feature mais significativa\n",
    "4) Roda modelos com a feature selecionada e adicionando mais uma\n",
    "5) Escolhe o melhor modelo\n",
    "6) Repete processo 4 e 5 até acabarem as features\n",
    "\n",
    "<img src=\"https://quantifyinghealth.com/wp-content/uploads/2019/10/forward-stepwise-algorithm.png\" text=\"https://quantifyinghealth.com/stepwise-selection/\" width=400>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__Backward Selection__\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Roda um modelo com todas as features\n",
    "2) Escolhe a feature menos significativa e elimina ela\n",
    "3) Roda um modelo com as features restantes\n",
    "4) Repete o processo 2 e 3 sequencialmente\n",
    "\n",
    "A vantagem do Backward Selection é considerar a interação entre as features antes de eliminá-las, mas se o número de features for muito grande a seleção pode demorar demais.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Bi-directional Elimination__\n",
    "\n",
    "Muito semelhante ao Foward selection, mas ao adicionar uma nova variável ele verifica a importantância de todas as features e se encontrar alguma com significância menor que a determinada previamente, remove essa feature específica por meio do Backward Elimination.\n",
    "\n",
    "Portanto, é uma combinação de seleção para frente e eliminação para trás.\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Execute a próxima etapa do Foward Selection.\n",
    "\n",
    "2) Execute todas as etapas de eliminação para trás. Ou seja, qualquer recurso adicionado anteriormente com p-value > significancia será removido do modelo).\n",
    "\n",
    "3) Repita as etapas 2 e 3 até obtermos um conjunto final ótimo de recursos.\n",
    "\n",
    "\n",
    "Para saber mais acesse o [link 1](https://quantifyinghealth.com/stepwise-selection/) e [link 2](https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE\n",
    "\n",
    "Conheceremos agora o método **Recursive Feature Elimination** (RFE).\n",
    "\n",
    "O RFE é um método que se utiliza de um estimador capaz de atribuir um score de **importância** a cada uma das features.\n",
    "\n",
    "> Por exemplo, podemos olhar para os coeficientes de um modelo linear (`coef_`), ou então, para os scores de importância de features (`feature_importances_`). Esse método só irá funcionar se o estimador escolhido retorna `coef_` ou `feature_importances_`.\n",
    "\n",
    "O método então considera recursivamente **subconjuntos cada vez menores de features**, da seguinte maneira:\n",
    "\n",
    "- O estimador é treinado inicialmente com todas as features;\n",
    "- A importância de cada uma das features é calculada;\n",
    "- **As features menos importantes são retiradas do conjunto de features**;\n",
    "- O processo recomeça, até que o número  desejado de features seja alcançado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sendo assim, temos dois hiperparâmetros importantes na classe [sklearn.feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html):\n",
    "\n",
    "- `estimator`: o estimador que irá disponibilizar os scores de importância de features;\n",
    "- `n_features_to_select`: a quantidade de features que o subconjunto final terá.\n",
    "\n",
    "Na prática, podemos utilizar um gridsearch para otimizar estes dois hiperparâmetros, ou então utilizar a classe [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html), que determina o melhor número de features automaticamente.\n",
    "\n",
    "Vamos ver o método na prática!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:25:13.156721Z",
     "start_time": "2022-03-11T22:25:12.752801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos usar como estimador uma DT e escolher as 10 features mais importantes\n",
    "# importa classes do DT e RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# instancia e faz o fit com RFE\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rfe = RFE(estimator=dt, n_features_to_select=10)\n",
    "\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:25:13.487434Z",
     "start_time": "2022-03-11T22:25:13.467443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "        True, False,  True, False,  True,  True, False, False, False,\n",
       "       False, False,  True,  True, False, False, False,  True,  True,\n",
       "        True,  True, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# support_ retorna uma máscara com as features selecionadas\n",
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 19, 18, 17, 16, 15, 14, 12, 10,  1,  9,  1,  7,  1,  1, 20,  6,\n",
       "        5, 13, 11,  1,  1,  4,  3,  2,  1,  1,  1,  1,  8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ranking_ retorna a posição em que as features foram selecionadas\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>texture error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.07255</td>\n",
       "      <td>1.7400</td>\n",
       "      <td>27.85</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>13.36</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.28780</td>\n",
       "      <td>0.31860</td>\n",
       "      <td>0.14160</td>\n",
       "      <td>0.2660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.05502</td>\n",
       "      <td>3.8960</td>\n",
       "      <td>22.81</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>11.92</td>\n",
       "      <td>38.30</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.05768</td>\n",
       "      <td>2.5420</td>\n",
       "      <td>13.12</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>10.94</td>\n",
       "      <td>23.31</td>\n",
       "      <td>0.06542</td>\n",
       "      <td>0.03986</td>\n",
       "      <td>0.02222</td>\n",
       "      <td>0.2699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.06183</td>\n",
       "      <td>1.2850</td>\n",
       "      <td>17.26</td>\n",
       "      <td>0.005608</td>\n",
       "      <td>13.75</td>\n",
       "      <td>25.99</td>\n",
       "      <td>0.18390</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.08312</td>\n",
       "      <td>0.2744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.06270</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>13.17</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>12.32</td>\n",
       "      <td>16.18</td>\n",
       "      <td>0.15070</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.08750</td>\n",
       "      <td>0.2733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.06317</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>19.53</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>0.32990</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.06552</td>\n",
       "      <td>1.1610</td>\n",
       "      <td>133.00</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>25.93</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.41160</td>\n",
       "      <td>0.61210</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.2968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.06344</td>\n",
       "      <td>0.4336</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>15.34</td>\n",
       "      <td>16.35</td>\n",
       "      <td>0.24740</td>\n",
       "      <td>0.17590</td>\n",
       "      <td>0.08056</td>\n",
       "      <td>0.2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.06325</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>31.98</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>19.59</td>\n",
       "      <td>24.89</td>\n",
       "      <td>0.39340</td>\n",
       "      <td>0.50180</td>\n",
       "      <td>0.25430</td>\n",
       "      <td>0.3109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.08116</td>\n",
       "      <td>2.2610</td>\n",
       "      <td>27.48</td>\n",
       "      <td>0.012860</td>\n",
       "      <td>11.26</td>\n",
       "      <td>24.39</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.34860</td>\n",
       "      <td>0.09910</td>\n",
       "      <td>0.2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean fractal dimension  texture error  area error  smoothness error  \\\n",
       "469                 0.07255         1.7400       27.85          0.014590   \n",
       "561                 0.05502         3.8960       22.81          0.007594   \n",
       "443                 0.05768         2.5420       13.12          0.010720   \n",
       "362                 0.06183         1.2850       17.26          0.005608   \n",
       "271                 0.06270         0.5293       13.17          0.006472   \n",
       "..                      ...            ...         ...               ...   \n",
       "184                 0.06317         0.4956       19.53          0.003290   \n",
       "300                 0.06552         1.1610      133.00          0.006056   \n",
       "220                 0.06344         0.4336       17.40          0.004133   \n",
       "230                 0.06325         0.6790       31.98          0.005532   \n",
       "176                 0.08116         2.2610       27.48          0.012860   \n",
       "\n",
       "     worst radius  worst texture  worst compactness  worst concavity  \\\n",
       "469         13.36          25.40            0.28780          0.31860   \n",
       "561         11.92          38.30            0.05494          0.00000   \n",
       "443         10.94          23.31            0.06542          0.03986   \n",
       "362         13.75          25.99            0.18390          0.12550   \n",
       "271         12.32          16.18            0.15070          0.12750   \n",
       "..            ...            ...                ...              ...   \n",
       "184         17.80          28.03            0.32990          0.36300   \n",
       "300         25.93          26.24            0.41160          0.61210   \n",
       "220         15.34          16.35            0.24740          0.17590   \n",
       "230         19.59          24.89            0.39340          0.50180   \n",
       "176         11.26          24.39            0.29500          0.34860   \n",
       "\n",
       "     worst concave points  worst symmetry  \n",
       "469               0.14160          0.2660  \n",
       "561               0.00000          0.1566  \n",
       "443               0.02222          0.2699  \n",
       "362               0.08312          0.2744  \n",
       "271               0.08750          0.2733  \n",
       "..                    ...             ...  \n",
       "184               0.12260          0.3175  \n",
       "300               0.19800          0.2968  \n",
       "220               0.08056          0.2380  \n",
       "230               0.25430          0.3109  \n",
       "176               0.09910          0.2614  \n",
       "\n",
       "[398 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.loc[:, rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como utilizar o [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:35:10.780784Z",
     "start_time": "2022-03-11T22:35:09.602376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),\n",
       "      estimator=DecisionTreeClassifier(random_state=42), scoring='accuracy')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa classes do RFECV e StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# instancia StratifiedKFold\n",
    "fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# instancia e faz o fit com RFE\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rfecv = RFECV(dt, cv=fold, scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como na classe do cross validate, ele retorna um dicionário com os scores de cada split no atributo `cv_results_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:35:59.485218Z",
     "start_time": "2022-03-11T22:35:59.432247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.871877</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.864662</td>\n",
       "      <td>0.872180</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.924641</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.932141</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.924242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.929654</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.934685</td>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.917293</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.932198</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.917293</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.932198</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.934723</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.927204</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.924679</td>\n",
       "      <td>0.018255</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.929673</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.917293</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.927185</td>\n",
       "      <td>0.015277</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.927185</td>\n",
       "      <td>0.018613</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.922154</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.929711</td>\n",
       "      <td>0.021427</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.922154</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.922135</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.927166</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.919629</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.927166</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.937191</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.927185</td>\n",
       "      <td>0.018613</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.932198</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.932198</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.939736</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.922135</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.932179</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.927204</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.924660</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score  split0_test_score  split1_test_score  \\\n",
       "0          0.871877        0.005771           0.864662           0.872180   \n",
       "1          0.924641        0.022078           0.947368           0.894737   \n",
       "2          0.932141        0.006373           0.939850           0.932331   \n",
       "3          0.929654        0.015424           0.909774           0.947368   \n",
       "4          0.934685        0.012722           0.917293           0.947368   \n",
       "5          0.932198        0.012116           0.917293           0.932331   \n",
       "6          0.932198        0.010445           0.924812           0.924812   \n",
       "7          0.934723        0.014016           0.924812           0.924812   \n",
       "8          0.927204        0.021414           0.902256           0.924812   \n",
       "9          0.924679        0.018255           0.902256           0.924812   \n",
       "10         0.929673        0.009216           0.917293           0.932331   \n",
       "11         0.927185        0.015277           0.909774           0.924812   \n",
       "12         0.927185        0.018613           0.902256           0.932331   \n",
       "13         0.922154        0.015278           0.902256           0.924812   \n",
       "14         0.929711        0.021427           0.902256           0.932331   \n",
       "15         0.929692        0.019617           0.902256           0.939850   \n",
       "16         0.922154        0.015278           0.902256           0.924812   \n",
       "17         0.922135        0.019649           0.894737           0.939850   \n",
       "18         0.927166        0.012632           0.909774           0.932331   \n",
       "19         0.919629        0.012613           0.902256           0.924812   \n",
       "20         0.927166        0.017616           0.902256           0.939850   \n",
       "21         0.937191        0.003442           0.932331           0.939850   \n",
       "22         0.927185        0.018613           0.902256           0.932331   \n",
       "23         0.932198        0.016120           0.909774           0.939850   \n",
       "24         0.932198        0.010445           0.924812           0.924812   \n",
       "25         0.939736        0.012139           0.924812           0.939850   \n",
       "26         0.922135        0.014058           0.902256           0.932331   \n",
       "27         0.932179        0.005954           0.932331           0.924812   \n",
       "28         0.927204        0.021414           0.902256           0.924812   \n",
       "29         0.924660        0.016103           0.902256           0.932331   \n",
       "\n",
       "    split2_test_score  \n",
       "0            0.878788  \n",
       "1            0.931818  \n",
       "2            0.924242  \n",
       "3            0.931818  \n",
       "4            0.939394  \n",
       "5            0.946970  \n",
       "6            0.946970  \n",
       "7            0.954545  \n",
       "8            0.954545  \n",
       "9            0.946970  \n",
       "10           0.939394  \n",
       "11           0.946970  \n",
       "12           0.946970  \n",
       "13           0.939394  \n",
       "14           0.954545  \n",
       "15           0.946970  \n",
       "16           0.939394  \n",
       "17           0.931818  \n",
       "18           0.939394  \n",
       "19           0.931818  \n",
       "20           0.939394  \n",
       "21           0.939394  \n",
       "22           0.946970  \n",
       "23           0.946970  \n",
       "24           0.946970  \n",
       "25           0.954545  \n",
       "26           0.931818  \n",
       "27           0.939394  \n",
       "28           0.954545  \n",
       "29           0.939394  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rfecv.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:36:29.480197Z",
     "start_time": "2022-03-11T22:36:29.460207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para saber o número de features escolhidos via CV usamos o n_features_\n",
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x=df.index, y='mean_test_score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='mean_test_score'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Também podemos ver graficamente\n",
    "sns.lineplot(data=df, x=df.index+1, y=df['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(data=rfecv.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFECV do yellowbrick\n",
    "https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, podemos incluir o RFE como um passo da Pipeline e otimizar seus parâmetros com o grid search!\n",
    "\n",
    "Tentem fazer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:45:50.528916Z",
     "start_time": "2022-03-11T22:45:50.493936Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:09:17.336189Z",
     "start_time": "2022-03-10T01:07:56.122167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instancia RFE e Decision Tree\n",
    "\n",
    "\n",
    "# Cria pipeline\n",
    "\n",
    "\n",
    "# Define param_grid\n",
    "param_grid_ab = {\"rfe__n_features_to_select\" : range(1, X_train.shape[1]+1),\n",
    "                 \"rfe__estimator__max_depth\" : [5, 10, 50],\n",
    "                 \"model__max_depth\" : [10, 25, 50]\n",
    "                }\n",
    "\n",
    "# Instancia StratifiedKFold\n",
    "\n",
    "\n",
    "# Instancia RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Fit do RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebam que utilizamos o mesmo modelo para escolher as melhores features (DT) e para predizer nosso target. Não é obrigatório utilizarmos o mesmo, mas a escolha de features estará otimizada para o modelo usado no método wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:09:36.708956Z",
     "start_time": "2022-03-10T01:09:36.701961Z"
    }
   },
   "outputs": [],
   "source": [
    "# retorna melhores parâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:10:03.346735Z",
     "start_time": "2022-03-10T01:10:02.403565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printa métricas de classificação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos rodar o modelo sem RFE para comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:46:09.822022Z",
     "start_time": "2022-03-11T22:45:54.151861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance\n",
    "\n",
    "Neste método, utilizamos a função [`sklearn.inspection.permutation_importance()`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html), que vai criar permutações das features, mantendo um registro do score. O permutation_importance é definido como a **diminuição no score de um modelo quando uma única feature é embaralhada aleatoriamente. Este procedimento quebra a relação entre a feature e o target e utiliza a queda na pontuação do modelo como um indicativo de quanto o modelo depende dessa feature**.\n",
    "\n",
    "Por realizar diversas permutações, este método é mais custoso, mas tem a vantagem de eliminar o viés que features de alta cardinalidade carregam com o método baseado em impureza.\n",
    "\n",
    "Para maiores detalhes sobre o método, [clique aqui!](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)\n",
    "\n",
    "> Observação: este é um método que pode ser usado com qualquer estimador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:02.771114Z",
     "start_time": "2022-03-11T22:18:54.187540Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Faz o fit do DT\n",
    "\n",
    "\n",
    "# Calcula o permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:03.252918Z",
     "start_time": "2022-03-11T22:19:02.976994Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cria df a partir dos atributos\n",
    "df_perm = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:03.847510Z",
     "start_time": "2022-03-11T22:19:03.256915Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using permutation importance\")\n",
    "plt.barh(df_perm['features'], df_perm[\"importance\"], xerr=df_perm[\"std\"])\n",
    "plt.xlabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________\n",
    "<br>\n",
    "\n",
    "### 3. Metodologia Híbrida\n",
    "\n",
    "Nessa metodologia o intuito é definir as principais features com um método de filtro e depois aplicar o método de wrapper para reduzir o espaço de features. Dessa forma, reduzimos a quantidade de testes a serem realizados por este último.\n",
    "<br><br><br>\n",
    "_______________________________________________________________________________________\n",
    "<br>\n",
    "\n",
    "### 4. Metodologia Embutida/Intrínseca (Embedded/Intrinsic)\n",
    "\n",
    "Nessa metodologia o próprio estimador possui uma forma de seleção de features. Os principais exemplos são métodos de árvore e regressão Lasso. \n",
    "No primeiro, a árvore seleciona uma feature em cada divisão, deixando por último as features menos relevantes. No segundo, as features menos relevantes têm o seu coeficiente zerado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO\n",
    "\n",
    "Já conhecemos um método capaz de realizar feature selection: a **regularização L1 (LASSO)**.\n",
    "\n",
    "Diferente da regularização L2, quando utilizamos regularização L1 é possível zerar alguns dos parâmetros do modelo:\n",
    "\n",
    "<img src=https://ugc.futurelearn.com/uploads/assets/2b/fe/2bfe399e-503e-4eae-9138-a3d7da738713.png width=800>\n",
    "\n",
    "Embora ambas as modalidades de regularização tenham sido introduzidas com o intuito de simplificar o espaço de hipóteses, o LASSO faz isso de maneira explícita, efetivamente possibilitando a realização de feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, há um problema: são poucos os métodos que têm o LASSO incorporado (ex.: regressão linear, logística, XGBoost).\n",
    "\n",
    "Assim, se quisermos realizar feature selection utilizando outros estimadores, precisamos de técnicas mais genéricas, que foi o que vimos.\n",
    "\n",
    "Para utilizarmos o L1, uma abordagem possível é:\n",
    "\n",
    "- **treinar inicialmente um modelo com LASSO**; \n",
    "- identificar quais features **ainda estão presentes no modelo** (isto é, com `coef_` não nulo);\n",
    "- utilizar apenas estas features para treinar o estimador desejado.\n",
    "\n",
    "Vamos ver como o Lasso se comporta em um dataset conhecido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale das features é obrigatório já que faremos uma regressão linear\n",
    "sc = MinMaxScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um loop que calculará o valor dos coeficientes e a métrica de erro conforme aumentamos o valor da regularização $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas = []\n",
    "coefs = []\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "\n",
    "for alpha in np.arange(0, 0.002, 0.00001):\n",
    "    # Cria uma instância do Lasso Regression\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    \n",
    "    # Fit Lasso model\n",
    "    lasso.fit(X_train_sc, y_train)\n",
    "    \n",
    "    # Salva os scores do modelo (nesse caso é o coeficiente de determinação R²)\n",
    "    train_scores.append(lasso.score(X_train_sc, y_train))\n",
    "    test_scores.append(lasso.score(X_test_sc, y_test))\n",
    "\n",
    "    # Salva o valor de alpha usado\n",
    "    alphas.append(alpha)\n",
    "\n",
    "    # Salva o valor dos coeficientes estimados\n",
    "    coefs.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena os valores de alpha e dos coeficientes em uma única lista\n",
    "concat_data = [np.append(alphas[i], coefs[i]) for i in range(len(alphas))]\n",
    "\n",
    "# Cria dataframe com um valor de lambda por linha e as colunas como valores dos coeficientes\n",
    "df = pd.DataFrame(concat_data, columns=['lambda']+bc.feature_names.tolist())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui já podemos ver que alguns coeficientes foram zerados pela regularização. Vamos olhar o heatmap de correlação entre as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df.drop('lambda', axis=1).corr(), method=\"complete\", cmap='RdBu', annot=True, \n",
    "               annot_kws={\"size\": 7}, vmin=-1, vmax=1, figsize=(15,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos muitas variáveis independentes correlacionadas e já sabemos que isso impacta no nosso modelo de regressão linear. Lasso escolhe aleatóriamente uma das variáveis multicolineares e zera as demais. Isso pode impactar na interpretabilidade do nosso modelo.\n",
    "\n",
    "Vamos ver como fica o coeficiente de determinação tanto para o treino quanto para o teste conforme aumentamos nossa regularização $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(x=alphas, y=train_scores, label='Train')\n",
    "sns.lineplot(x=alphas, y=test_scores, label='Teste')\n",
    "plt.ylabel('Coeficiente de determinação (R²)')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual o impacto da regularização Lasso no erro de predição do treino e do teste?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme esperado, o R² do treino diminui com o aumento da regularização enquanto o do teste atinge um plato próximo de 0.75 e depois começa a cair. Nosso valor ideal de regularização está próximo de $\\lambda = 0.00015$.\n",
    "\n",
    "Vamos ver agora como o aumento da regularização afeta os coeficientes das nossa features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = [-0.00001, 0.0012]\n",
    "figure_size = (15,7)\n",
    "df.plot(x='lambda', figsize=figure_size)\n",
    "plt.axvline(x=0.00015, linestyle='--')\n",
    "plt.xlim(x_lim)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.ylabel('Coeficientes encontrados')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual o impacto da regularização Lasso nos coeficientes?', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos ver quantas features sobram conforme aumentamos a regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_features_zeradas = df.groupby('lambda').agg(lambda x: x.ne(0).sum()).sum(axis=1).reset_index()\n",
    "plt.figure(figsize=figure_size)\n",
    "sns.lineplot(data=qtd_features_zeradas, x='lambda', y=0)\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim([0, 33])\n",
    "plt.ylabel('Coeficientes não zerados')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual a quantidade de coeficientes zerados com o aumento da regularização Lasso?', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse gráfico percebemos que algumas variáveis vão a zero muito rapidamente\n",
    "\n",
    "[link](https://afit-r.github.io/regularized_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance com árvores\n",
    "\n",
    "Além de estimadores poderosos, podemos utilizar modelos baseados em árvores para fazer feature selection! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.feature_importances_`\n",
    "\n",
    "Neste caso, o score de importância de cada uma das features é calculado com base na **média e desvio padrão da diminuição de impureza que cada feature proporciona na árvore (ou em cada árvore, no caso de ensembles)**.\n",
    "\n",
    "O método é conhecido como **mean decrease in impurity** (MDI).\n",
    "\n",
    "Este método é rápido, no entanto, o valor é fortemente enviesado para features que têm alta cardinalidade (features numéricas, ou features categóricas com muitos níveis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:51.657887Z",
     "start_time": "2022-03-11T22:18:48.060844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instancia e faz o fit do RF\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:52.475678Z",
     "start_time": "2022-03-11T22:18:52.227797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cria series com o nome da feature e o importance\n",
    "feature_importances_rf = pd.Series(rf.feature_importances_, index=rf.feature_names_in_).sort_values(ascending=False)\n",
    "feature_importances_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:53.026387Z",
     "start_time": "2022-03-11T22:18:52.478676Z"
    }
   },
   "outputs": [],
   "source": [
    "# plota o feature importance\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using MDI\")\n",
    "plt.barh(feature_importances_rf.index, feature_importances_rf.values)\n",
    "plt.xlabel(\"Mean decrease in impurity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, o próximo passo seria treinar um modelo normalmente (pipeline, gridsearch, etc.), mas apenas com essas features selecionadas (idealmente, uma combinação dessas heurísticas).\n",
    "\n",
    "Nota: Sempre verifique se você obtém os mesmos resultados com um random_seed diferente antes de interpretar qualquer classificação de importância. Se os resultados mudarem, aumente o número de árvores com `ntree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual método é melhor?\n",
    "Como um bom cientista de dados, você terá que realizar vários experimentos para descobrir qual método é melhor para o seu caso!\n",
    "\n",
    "Mas você pode também usar alguns métodos e selecionar todas as features que foram consideradas importantes por pelo menos 1 ou 2 deles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________\n",
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "- [Breve introdução dos métodos supervisionados e não supervisionados de seleção de features](https://www.kdnuggets.com/2021/06/feature-selection-overview.html)\n",
    "- [Lista de estratégias](https://towardsdatascience.com/feature-selection-a-comprehensive-list-of-strategies-3fecdf802b79)\n",
    "- [MUITOS métodos diferentes](https://medium.com/analytics-vidhya/feature-selection-extended-overview-b58f1d524c1c)\n",
    "- [Wrapper methods](https://quantifyinghealth.com/stepwise-selection/)\n",
    "- [Understanding Bias in RF Variable Importance Metrics](https://blog.methodsconsultants.com/posts/be-aware-of-bias-in-rf-variable-importance-metrics/)\n",
    "- [Feature Importance x Permutation Importance](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material Complementar: Técnicas Supervisionadas\n",
    "\n",
    "#### Como encontrar o número ideal de features?\n",
    "Método 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load needed libraries\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import and prepare data\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "#Define Sequential Forward Selection (sfs)\n",
    "sfs = SFS(LinearRegression(), \n",
    "          k_features=13, \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='neg_mean_squared_error',\n",
    "          cv=5)\n",
    "\n",
    "sfs = sfs.fit(X, y)\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#https://towardsdatascience.com/feature-selection-for-machine-learning-in-python-wrapper-methods-2b5e27d2db31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método 2:\n",
    "\n",
    "Adicione uma variável randômica no seu dataset e faça o feature importance. Qualquer feature que tenha uma importância menor que a da feature randômica pode ser desconsiderada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "X_train['random_1'] = np.random.randint(-100, 100, size=X_train.shape[0])/100\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "\n",
    "feature_importances_rf = pd.Series(rf.feature_importances_, index=rf.feature_names_in_).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using MDI\")\n",
    "plt.barh(feature_importances_rf.index, feature_importances_rf.values)\n",
    "plt.xlabel(\"Mean decrease in impurity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material Complementar: Técnicas Não Supervisionadas\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/Fig4-Butvinik-feature-selection-overview.jpg\" text=\"https://www.kdnuggets.com/2021/06/feature-selection-overview.html\" width=900/>\n",
    "\n",
    "Aqui também podemos separar nossos métodos de acordo com a interação com o modelo:\n",
    "\n",
    "### 1. Filtro\n",
    "\n",
    "Os métodos de filtro podem ser classificados como univariados e multivariados. Da mesma forma que nas técnicas supervisionadas, aqui ordenamos as features de acordo com o método estatístico selecionado. No caso dos univariados, cada feature é analisada separadamente e por isso não é capaz de eliminar features correlacionadas. No multivariado, as features são avaliadas em conjunto em vez de individualmente. \n",
    "\n",
    "### 2. Wrapper\n",
    "\n",
    "O métodos wrapper podem ser divididos em três categorias: sequencial, bio-inspirado e iterativo. \n",
    "\n",
    "Na metodologia sequencial, os recursos são adicionados ou removidos sequencialmente. Na bioinspirada, tenta-se incorporar a aleatoriedade no processo de busca, com o intuito de escapar de ótimos locais. Nos iterativos fazemos uma estimação evitando, assim, uma busca combinatória.\n",
    "\n",
    "Assim como nos métodos wrapper supervisionados, esta abordagem caracteriza-se por encontrar subconjuntos de features que sejam mais relevantes. Porém, ela possui um alto custo computacional.\n",
    "\n",
    "### 3. Metodologia Híbrida\n",
    "\n",
    "Da mesma forma que nos supervisionados, a primeira etapa dessa metodologia consiste em filtrar os recursos mais relevantes para depois aplicar alguma técnica de wrapper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb5f626699f206ef97176a4f092b8d9f6e52ae1f84b4bb3163daf9eb25ca3519"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aula_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
